# Neural_network_SBU_HW1
# One layer hidden neural network implemented by using Numpy trained on sklearned make_moons
In this homework our objective was to develop this network to deepen our understanding of neural network mathemathical base.
<br>
Output activation functions: Sigmoid,Tanh,Linear
<br>
Hidden activation functions: Sigmoid,Tanh
<br>
In this code both type of learning is available and you can use either batch gradient descent or online gradient descent.
<br>
Momentum and decay rate have been used.
<p align="center">
  <img width="460" height="300" src="https://github.com/smrh1379/Neural_network_SBU/blob/Homework_1/results/MSE_batch_sigmoid.jpeg?raw=true">
</p>
<p align="center">
  MSE_batch_sigmoid
</p>
<p align="center">
  <img width="460" height="300" src="https://github.com/smrh1379/Neural_network_SBU/blob/Homework_1/results/plot_batch_sigmoid.jpeg">
</p>
<p align="center">
  Plot_batch_sigmoid
</p>
<br>
Each way of training have been used and MSE and plots are available in the results folder.
